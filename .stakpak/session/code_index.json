{
  "last_updated": "2025-07-21T20:03:17.577301Z",
  "index": {
    "blocks": [
      {
        "id": "9f7b03dd-e3c2-4c48-ad18-c1479830a2dc",
        "provider": "GithubActions",
        "provisioner": "GithubActions",
        "language": "yaml",
        "key": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/lighthouse-ci.yml:lighthouse-ci---accessibility-monitoring",
        "digest": 8942428048262580549,
        "references": [],
        "kind": "workflow",
        "type": null,
        "name": "Lighthouse CI - Accessibility Monitoring",
        "config": null,
        "document_uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/lighthouse-ci.yml",
        "code": "name: Lighthouse CI - Accessibility Monitoring\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n  schedule:\n    # Run daily at 2 AM UTC for continuous monitoring\n    - cron: '0 2 * * *'\n\njobs:\n  lighthouse:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        page:\n          [\n            { name: 'Home', path: '/', component: 'Landing' },\n            {\n              name: 'Components',\n              path: '/components',\n              component: 'ComponentShowcase',\n            },\n            {\n              name: 'Playground',\n              path: '/playground',\n              component: 'Interactive',\n            },\n            { name: 'Forms', path: '/forms', component: 'FormComponents' },\n            {\n              name: 'Navigation',\n              path: '/navigation',\n              component: 'NavigationComponents',\n            },\n          ]\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build project\n        run: npm run build\n\n      - name: Build Storybook for testing\n        run: npm run build-storybook\n\n      - name: Install Lighthouse CI and dependencies\n        run: |\n          npm install -g @lhci/cli@0.13.x\n          npm install -g http-server\n\n      - name: Start test server\n        run: |\n          http-server ./storybook-static -p 3000 --silent &\n          sleep 10\n          curl -f http://localhost:3000 || exit 1\n\n      - name: Run Lighthouse CI - ${{ matrix.page.name }}\n        run: |\n          lhci autorun --config=lighthouserc.js\n        env:\n          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}\n          LHCI_BUILD_CONTEXT__CURRENT_HASH: ${{ github.sha }}\n          LHCI_BUILD_CONTEXT__COMMIT_MESSAGE: ${{ github.event.head_commit.message }}\n          PAGE_PATH: ${{ matrix.page.path }}\n          PAGE_NAME: ${{ matrix.page.name }}\n\n      - name: Parse Lighthouse results\n        id: lighthouse-results\n        run: |\n          # Parse the manifest.json to extract scores\n          if [ -f \".lighthouseci/manifest.json\" ]; then\n            MANIFEST=$(cat .lighthouseci/manifest.json)\n            ACCESSIBILITY_SCORE=$(echo $MANIFEST | jq -r '.[0].summary.accessibility')\n            PERFORMANCE_SCORE=$(echo $MANIFEST | jq -r '.[0].summary.performance')\n            BEST_PRACTICES_SCORE=$(echo $MANIFEST | jq -r '.[0].summary[\"best-practices\"]')\n            SEO_SCORE=$(echo $MANIFEST | jq -r '.[0].summary.seo')\n\n            # Convert to percentage\n            ACCESSIBILITY_PCT=$(echo \"$ACCESSIBILITY_SCORE * 100\" | bc -l | cut -d'.' -f1)\n            PERFORMANCE_PCT=$(echo \"$PERFORMANCE_SCORE * 100\" | bc -l | cut -d'.' -f1)\n            BEST_PRACTICES_PCT=$(echo \"$BEST_PRACTICES_SCORE * 100\" | bc -l | cut -d'.' -f1)\n            SEO_PCT=$(echo \"$SEO_SCORE * 100\" | bc -l | cut -d'.' -f1)\n\n            echo \"accessibility_score=$ACCESSIBILITY_PCT\" >> $GITHUB_OUTPUT\n            echo \"performance_score=$PERFORMANCE_PCT\" >> $GITHUB_OUTPUT\n            echo \"best_practices_score=$BEST_PRACTICES_PCT\" >> $GITHUB_OUTPUT\n            echo \"seo_score=$SEO_PCT\" >> $GITHUB_OUTPUT\n\n            # Check if accessibility meets S-tier requirement (95%+)\n            if [ $ACCESSIBILITY_PCT -ge 95 ]; then\n              echo \"accessibility_status=✅ PASSED\" >> $GITHUB_OUTPUT\n            else\n              echo \"accessibility_status=❌ FAILED\" >> $GITHUB_OUTPUT\n            fi\n\n            # Generate detailed accessibility report\n            echo \"## 🌟 S-Tier Accessibility Analysis - ${{ matrix.page.name }}\" > accessibility-report.md\n            echo \"\" >> accessibility-report.md\n            echo \"| Metric | Score | Status |\" >> accessibility-report.md\n            echo \"|--------|-------|--------|\" >> accessibility-report.md\n            echo \"| Accessibility | ${ACCESSIBILITY_PCT}% | $([ $ACCESSIBILITY_PCT -ge 95 ] && echo '✅ S-tier' || echo '❌ Below target') |\" >> accessibility-report.md\n            echo \"| Performance | ${PERFORMANCE_PCT}% | $([ $PERFORMANCE_PCT -ge 90 ] && echo '✅ Excellent' || echo '⚠️ Needs improvement') |\" >> accessibility-report.md\n            echo \"| Best Practices | ${BEST_PRACTICES_PCT}% | $([ $BEST_PRACTICES_PCT -ge 90 ] && echo '✅ Excellent' || echo '⚠️ Needs improvement') |\" >> accessibility-report.md\n            echo \"| SEO | ${SEO_PCT}% | $([ $SEO_PCT -ge 90 ] && echo '✅ Excellent' || echo '⚠️ Needs improvement') |\" >> accessibility-report.md\n            echo \"\" >> accessibility-report.md\n\n            if [ $ACCESSIBILITY_PCT -ge 95 ]; then\n              echo \"🎉 **S-Tier Status**: Accessibility target achieved!\" >> accessibility-report.md\n            else\n              echo \"⚠️ **Action Required**: Accessibility score below 95% S-tier target\" >> accessibility-report.md\n              echo \"\" >> accessibility-report.md\n              echo \"### Accessibility Issues to Address:\" >> accessibility-report.md\n\n              # Extract specific accessibility violations from the JSON report\n              if [ -f \".lighthouseci/lhr-*.json\" ]; then\n                LATEST_REPORT=$(ls -t .lighthouseci/lhr-*.json | head -1)\n                cat $LATEST_REPORT | jq -r '.audits | to_entries[] | select(.value.score != null and .value.score < 1 and (.key | contains(\"accessibility\") or .key | contains(\"aria\") or .key | contains(\"color\") or .key | contains(\"focus\"))) | \"- **\\(.key)**: \\(.value.title) (Score: \\(.value.score))\"' >> accessibility-report.md || true\n              fi\n            fi\n          else\n            echo \"❌ Lighthouse report not found\" >> accessibility-report.md\n          fi\n\n      - name: Extract accessibility violations\n        if: steps.lighthouse-results.outputs.accessibility_score < 95\n        run: |\n          # Create detailed accessibility violation report\n          echo \"## 🔍 Detailed Accessibility Analysis\" > violations-${{ matrix.page.name }}.md\n          echo \"\" >> violations-${{ matrix.page.name }}.md\n\n          if [ -f \".lighthouseci/lhr-*.json\" ]; then\n            LATEST_REPORT=$(ls -t .lighthouseci/lhr-*.json | head -1)\n\n            # Extract color contrast issues\n            echo \"### Color Contrast Issues\" >> violations-${{ matrix.page.name }}.md\n            cat $LATEST_REPORT | jq -r '.audits[\"color-contrast\"] | if .details and .details.items then .details.items[] | \"- Element: `\\(.node.snippet // .node.selector)` - Contrast ratio: \\(.contrastRatio)\" else \"No color contrast issues found\" end' >> violations-${{ matrix.page.name }}.md || echo \"No color contrast data available\" >> violations-${{ matrix.page.name }}.md\n            echo \"\" >> violations-${{ matrix.page.name }}.md\n\n            # Extract ARIA issues\n            echo \"### ARIA Issues\" >> violations-${{ matrix.page.name }}.md\n            cat $LATEST_REPORT | jq -r '.audits | to_entries[] | select(.key | contains(\"aria\")) | select(.value.score != null and .value.score < 1) | \"- **\\(.key)**: \\(.value.title)\"' >> violations-${{ matrix.page.name }}.md || echo \"No ARIA issues found\" >> violations-${{ matrix.page.name }}.md\n            echo \"\" >> violations-${{ matrix.page.name }}.md\n\n            # Extract keyboard navigation issues\n            echo \"### Keyboard Navigation Issues\" >> violations-${{ matrix.page.name }}.md\n            cat $LATEST_REPORT | jq -r '.audits | to_entries[] | select(.key | contains(\"focus\") or .key | contains(\"tabindex\")) | select(.value.score != null and .value.score < 1) | \"- **\\(.key)**: \\(.value.title)\"' >> violations-${{ matrix.page.name }}.md || echo \"No keyboard navigation issues found\" >> violations-${{ matrix.page.name }}.md\n          fi\n\n      - name: Upload Lighthouse reports\n        uses: actions/upload-artifact@v4\n        with:\n          name: lighthouse-reports-${{ matrix.page.name }}\n          path: |\n            .lighthouseci/\n            accessibility-report.md\n            violations-${{ matrix.page.name }}.md\n          retention-days: 30\n\n      - name: Store accessibility metrics\n        run: |\n          # Store metrics for trend analysis\n          mkdir -p .accessibility-metrics\n          echo \"{ \\\"date\\\": \\\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\", \\\"page\\\": \\\"${{ matrix.page.name }}\\\", \\\"accessibility\\\": ${{ steps.lighthouse-results.outputs.accessibility_score }}, \\\"performance\\\": ${{ steps.lighthouse-results.outputs.performance_score }}, \\\"commit\\\": \\\"${{ github.sha }}\\\" }\" > .accessibility-metrics/${{ matrix.page.name }}-$(date +%Y%m%d-%H%M%S).json\n\n      - name: Upload accessibility metrics\n        uses: actions/upload-artifact@v4\n        with:\n          name: accessibility-metrics-${{ matrix.page.name }}\n          path: .accessibility-metrics/\n          retention-days: 90\n\n  accessibility-summary:\n    needs: lighthouse\n    runs-on: ubuntu-latest\n    if: always()\n    steps:\n      - name: Download all accessibility reports\n        uses: actions/download-artifact@v4\n        with:\n          pattern: lighthouse-reports-*\n          merge-multiple: true\n\n      - name: Download all accessibility metrics\n        uses: actions/download-artifact@v4\n        with:\n          pattern: accessibility-metrics-*\n          merge-multiple: true\n\n      - name: Generate comprehensive accessibility report\n        run: |\n          echo \"# 🌟 LiqUIdify S-Tier Accessibility Report\" > comprehensive-accessibility-report.md\n          echo \"\" >> comprehensive-accessibility-report.md\n          echo \"**Generated**: $(date -u)\" >> comprehensive-accessibility-report.md\n          echo \"**Commit**: ${{ github.sha }}\" >> comprehensive-accessibility-report.md\n          echo \"**Target**: 95%+ Accessibility Score (S-Tier)\" >> comprehensive-accessibility-report.md\n          echo \"\" >> comprehensive-accessibility-report.md\n\n          # Combine all individual reports\n          for report in accessibility-report.md; do\n            if [ -f \"$report\" ]; then\n              cat \"$report\" >> comprehensive-accessibility-report.md\n              echo \"\" >> comprehensive-accessibility-report.md\n            fi\n          done\n\n          # Add trend analysis if metrics exist\n          if [ -d \".accessibility-metrics\" ] && [ \"$(ls -A .accessibility-metrics)\" ]; then\n            echo \"## 📈 Accessibility Trends\" >> comprehensive-accessibility-report.md\n            echo \"\" >> comprehensive-accessibility-report.md\n            echo \"| Page | Current Score | Trend |\" >> comprehensive-accessibility-report.md\n            echo \"|------|---------------|-------|\" >> comprehensive-accessibility-report.md\n\n            for page in Home Components Playground Forms Navigation; do\n              LATEST_SCORE=$(find .accessibility-metrics -name \"${page}-*.json\" | sort | tail -1 | xargs cat 2>/dev/null | jq -r '.accessibility' || echo \"N/A\")\n              if [ \"$LATEST_SCORE\" != \"N/A\" ]; then\n                if [ \"$LATEST_SCORE\" -ge 95 ]; then\n                  TREND=\"✅ S-tier\"\n                elif [ \"$LATEST_SCORE\" -ge 90 ]; then\n                  TREND=\"⚠️ Close to target\"\n                else\n                  TREND=\"❌ Needs improvement\"\n                fi\n                echo \"| $page | ${LATEST_SCORE}% | $TREND |\" >> comprehensive-accessibility-report.md\n              fi\n            done\n          fi\n\n      - name: Upload comprehensive report\n        uses: actions/upload-artifact@v4\n        with:\n          name: comprehensive-accessibility-report\n          path: comprehensive-accessibility-report.md\n          retention-days: 90\n\n      - name: Comment PR with accessibility results\n        if: github.event_name == 'pull_request'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            let comment = '';\n\n            try {\n              const report = fs.readFileSync('comprehensive-accessibility-report.md', 'utf8');\n              comment = report;\n            } catch (error) {\n              comment = `## 🌟 Accessibility Monitoring\n\n              ⚠️ Unable to generate comprehensive report. Please check individual page reports in the workflow artifacts.\n\n              **S-Tier Target**: 95%+ Accessibility Score\n\n              [View detailed results](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;\n            }\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: comment\n            });\n\n      - name: Fail if accessibility targets not met\n        run: |\n          # Check if any page failed the 95% accessibility target\n          FAILED_PAGES=\"\"\n          for report in accessibility-report.md; do\n            if [ -f \"$report\" ] && grep -q \"❌ FAILED\" \"$report\"; then\n              PAGE=$(echo \"$report\" | sed 's/accessibility-report-\\(.*\\)\\.md/\\1/')\n              FAILED_PAGES=\"$FAILED_PAGES $PAGE\"\n            fi\n          done\n\n          if [ -n \"$FAILED_PAGES\" ]; then\n            echo \"❌ Accessibility targets not met for pages:$FAILED_PAGES\"\n            echo \"S-Tier requirement: 95%+ accessibility score\"\n            exit 1\n          else\n            echo \"✅ All pages meet S-Tier accessibility requirements!\"\n          fi\n\n  accessibility-monitoring:\n    needs: accessibility-summary\n    runs-on: ubuntu-latest\n    if: github.event_name == 'schedule' # Only run on scheduled builds for monitoring\n    steps:\n      - name: Send accessibility alert\n        if: failure()\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const issue = {\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              title: '🚨 Accessibility Monitoring Alert - S-Tier Standards Not Met',\n              body: `## Automated Accessibility Monitoring Alert\n\n              **Date**: ${new Date().toISOString()}\n              **Build**: [${context.runId}](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n\n              ⚠️ The scheduled accessibility monitoring detected that one or more pages have fallen below the 95% S-tier accessibility target.\n\n              ### Required Actions:\n              1. Review the detailed accessibility report in the workflow artifacts\n              2. Address the identified accessibility violations\n              3. Test fixes with screen readers and keyboard navigation\n              4. Ensure all components maintain WCAG 2.1 AA compliance\n\n              ### Resources:\n              - [Accessibility Guidelines](./docs/accessibility.md)\n              - [WCAG 2.1 Quick Reference](https://www.w3.org/WAI/WCAG21/quickref/)\n              - [Testing with Screen Readers](./docs/testing-accessibility.md)\n\n              This issue will be automatically closed when accessibility scores return to S-tier levels.`,\n              labels: ['accessibility', 'priority-high', 's-tier', 'automated']\n            };\n\n            github.rest.issues.create(issue);\n",
        "start_byte": 0,
        "end_byte": 15163,
        "start_point": {
          "row": 0,
          "column": 0
        },
        "end_point": {
          "row": 329,
          "column": 0
        },
        "state": null,
        "updated_at": null,
        "created_at": null,
        "dependents": [],
        "dependencies": [],
        "api_group_version": {
          "alias": "github-actions",
          "group": "github/actions",
          "version": "1.0.0",
          "provisioner": "GithubActions",
          "status": "AVAILABLE"
        },
        "generated_summary": null
      },
      {
        "id": "9d98234b-8ce1-474b-bcab-6b0257f25cbf",
        "provider": "GithubActions",
        "provisioner": "GithubActions",
        "language": "yaml",
        "key": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/performance-monitoring.yml:performance-monitoring",
        "digest": 1383624688427645039,
        "references": [],
        "kind": "workflow",
        "type": null,
        "name": "Performance Monitoring",
        "config": null,
        "document_uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/performance-monitoring.yml",
        "code": "name: Performance Monitoring\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n  schedule:\n    # Run performance tests daily at 3 AM UTC\n    - cron: '0 3 * * *'\n\njobs:\n  performance-benchmarks:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [18, 20]\n        browser: [chrome, firefox]\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build production bundle\n        run: npm run build:production\n\n      - name: Setup Chrome for performance testing\n        if: matrix.browser == 'chrome'\n        uses: browser-actions/setup-chrome@latest\n        with:\n          chrome-version: stable\n\n      - name: Setup Firefox for performance testing\n        if: matrix.browser == 'firefox'\n        uses: browser-actions/setup-firefox@latest\n        with:\n          firefox-version: latest\n\n      - name: Run component performance benchmarks\n        run: |\n          npm run perf:benchmark -- --ci --browser=${{ matrix.browser }}\n        env:\n          NODE_ENV: production\n          PERFORMANCE_BUDGET_ENFORCE: true\n\n      - name: Run memory leak detection\n        run: |\n          npm run perf:memory-leak -- --ci --browser=${{ matrix.browser }}\n\n      - name: Run real-world performance simulation\n        run: |\n          npm run perf:real-world -- --ci --browser=${{ matrix.browser }}\n\n      - name: Upload performance reports\n        uses: actions/upload-artifact@v4\n        with:\n          name: performance-reports-node${{ matrix.node-version }}-${{ matrix.browser }}\n          path: |\n            performance-reports/\n            dist/performance-*.json\n          retention-days: 30\n\n      - name: Comment PR with performance results\n        if: github.event_name == 'pull_request'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            const path = require('path');\n\n            // Read performance results\n            const reportPath = 'performance-reports/benchmark-results.json';\n            if (!fs.existsSync(reportPath)) {\n              console.log('No performance report found');\n              return;\n            }\n\n            const results = JSON.parse(fs.readFileSync(reportPath, 'utf8'));\n\n            // Generate performance comment\n            const comment = `## 🚀 Performance Report - Node ${{ matrix.node-version }} (${{ matrix.browser }})\n\n            | Component | Render Time | Memory Leak | Frame Rate | Status |\n            |-----------|-------------|-------------|------------|--------|\n            ${results.results.map(result =>\n              `| ${result.name} | ${result.renderTime.average.toFixed(2)}ms | ${(result.memoryUsage.leaked / 1024).toFixed(2)}KB | ${result.frameRate.average.toFixed(1)}fps | ${result.passed ? '✅' : '❌'} |`\n            ).join('\\n')}\n\n            ### S-Tier Performance Targets\n            - **Render Time**: <16ms (60fps)\n            - **Memory Leak**: <1MB\n            - **Frame Rate**: >55fps\n            - **Bundle Size**: <30KB\n\n            ### Summary\n            - **Total Tests**: ${results.summary.total}\n            - **Passed**: ${results.summary.passed}\n            - **Pass Rate**: ${results.summary.passRate}%\n\n            ${results.summary.passed === results.summary.total\n              ? '🎉 All performance targets met!'\n              : '⚠️ Some performance targets not met. Please review and optimize.'}\n\n            [View detailed performance report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: comment\n            });\n\n  bundle-size-performance:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build and analyze bundle performance\n        run: |\n          npm run build:production\n          npm run bundle:budget:enforce\n\n      - name: Run tree-shaking verification\n        run: |\n          npm run bundle:tree-shake:verify\n\n      - name: Analyze bundle size impact\n        id: bundle-analysis\n        run: |\n          # Run bundle analysis and capture metrics\n          CORE_SIZE=$(stat -c%s dist/core.mjs 2>/dev/null || echo \"0\")\n          ANIMATIONS_SIZE=$(stat -c%s dist/animations.mjs 2>/dev/null || echo \"0\")\n          ADVANCED_SIZE=$(stat -c%s dist/advanced.mjs 2>/dev/null || echo \"0\")\n          TOTAL_SIZE=$((CORE_SIZE + ANIMATIONS_SIZE + ADVANCED_SIZE))\n\n          # Convert to KB\n          CORE_KB=$((CORE_SIZE / 1024))\n          ANIMATIONS_KB=$((ANIMATIONS_SIZE / 1024))\n          ADVANCED_KB=$((ADVANCED_SIZE / 1024))\n          TOTAL_KB=$((TOTAL_SIZE / 1024))\n\n          echo \"core_size_kb=$CORE_KB\" >> $GITHUB_OUTPUT\n          echo \"animations_size_kb=$ANIMATIONS_KB\" >> $GITHUB_OUTPUT\n          echo \"advanced_size_kb=$ADVANCED_KB\" >> $GITHUB_OUTPUT\n          echo \"total_size_kb=$TOTAL_KB\" >> $GITHUB_OUTPUT\n\n          # Check S-Tier bundle requirements\n          if [ $CORE_KB -gt 15 ]; then\n            echo \"❌ Core bundle exceeds 15KB: ${CORE_KB}KB\"\n            exit 1\n          fi\n\n          if [ $ANIMATIONS_KB -gt 10 ]; then\n            echo \"❌ Animations bundle exceeds 10KB: ${ANIMATIONS_KB}KB\"\n            exit 1\n          fi\n\n          if [ $ADVANCED_KB -gt 8 ]; then\n            echo \"❌ Advanced bundle exceeds 8KB: ${ADVANCED_KB}KB\"\n            exit 1\n          fi\n\n          if [ $TOTAL_KB -gt 30 ]; then\n            echo \"❌ Total bundle exceeds 30KB: ${TOTAL_KB}KB\"\n            exit 1\n          fi\n\n          echo \"✅ All bundles within S-Tier limits\"\n\n      - name: Upload bundle analysis\n        uses: actions/upload-artifact@v4\n        with:\n          name: bundle-analysis\n          path: |\n            dist/bundle-*.json\n            dist/BUNDLE_BUDGET_REPORT.md\n          retention-days: 30\n\n  memory-leak-detection:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build for testing\n        run: npm run build\n\n      - name: Setup Chrome with memory flags\n        uses: browser-actions/setup-chrome@latest\n        with:\n          chrome-version: stable\n\n      - name: Run comprehensive memory leak detection\n        run: |\n          # Run with garbage collection exposed for accurate memory measurement\n          npm run perf:memory-leak:comprehensive -- \\\n            --cycles=20 \\\n            --components=all \\\n            --timeout=300000 \\\n            --gc-exposed\n        env:\n          NODE_OPTIONS: \"--expose-gc --max-old-space-size=4096\"\n\n      - name: Analyze memory growth patterns\n        run: |\n          node scripts/analyze-memory-patterns.js performance-reports/memory-leak-results.json\n\n      - name: Check memory leak thresholds\n        run: |\n          # Fail if any component has memory leak > 1MB\n          node -e \"\n            const results = require('./performance-reports/memory-leak-results.json');\n            const failures = results.components.filter(c => c.leakRate > 1024 * 1024);\n            if (failures.length > 0) {\n              console.log('❌ Memory leak detected in:', failures.map(f => f.name).join(', '));\n              process.exit(1);\n            }\n            console.log('✅ No significant memory leaks detected');\n          \"\n\n      - name: Upload memory analysis\n        uses: actions/upload-artifact@v4\n        with:\n          name: memory-leak-analysis\n          path: |\n            performance-reports/memory-*.json\n            performance-reports/memory-patterns.html\n          retention-days: 30\n\n  performance-regression-detection:\n    runs-on: ubuntu-latest\n    needs: [performance-benchmarks]\n    if: github.event_name == 'pull_request'\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Checkout main branch for comparison\n        run: |\n          git fetch origin main\n          git checkout main\n          npm ci\n          npm run build:production\n          npm run perf:benchmark -- --ci --output=main-baseline.json\n\n      - name: Checkout PR branch\n        run: |\n          git checkout ${{ github.head_ref }}\n          npm ci\n          npm run build:production\n          npm run perf:benchmark -- --ci --output=pr-results.json\n\n      - name: Compare performance metrics\n        id: compare\n        run: |\n          node scripts/compare-performance.js main-baseline.json pr-results.json > performance-comparison.md\n\n      - name: Check for performance regressions\n        run: |\n          # Check if any component has >10% performance regression\n          node -e \"\n            const fs = require('fs');\n            const comparison = JSON.parse(fs.readFileSync('performance-comparison.json'));\n\n            const regressions = comparison.regressions.filter(r => r.degradation > 10);\n            if (regressions.length > 0) {\n              console.log('❌ Performance regressions detected:');\n              regressions.forEach(r => {\n                console.log(\\`  \\${r.component}: \\${r.metric} degraded by \\${r.degradation}%\\`);\n              });\n              process.exit(1);\n            }\n            console.log('✅ No significant performance regressions');\n          \"\n\n      - name: Comment PR with regression analysis\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            const comparison = fs.readFileSync('performance-comparison.md', 'utf8');\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: comparison\n            });\n\n  real-world-performance-validation:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build production bundle\n        run: npm run build:production\n\n      - name: Setup Playwright for real-world testing\n        run: npx playwright install --with-deps chromium firefox webkit\n\n      - name: Run real-world performance scenarios\n        run: |\n          npm run perf:real-world -- \\\n            --scenarios=mobile-3g,mobile-4g,desktop,tablet \\\n            --users=1,5,10 \\\n            --duration=60 \\\n            --output=real-world-results.json\n\n      - name: Validate Core Web Vitals\n        run: |\n          node scripts/validate-core-web-vitals.js real-world-results.json\n\n      - name: Test with slow devices simulation\n        run: |\n          npm run perf:slow-devices -- \\\n            --cpu-throttling=4 \\\n            --memory-limit=512mb \\\n            --network=slow-3g\n\n      - name: Generate performance score\n        id: score\n        run: |\n          SCORE=$(node scripts/calculate-performance-score.js real-world-results.json)\n          echo \"performance_score=$SCORE\" >> $GITHUB_OUTPUT\n\n          if [ $SCORE -lt 85 ]; then\n            echo \"❌ Performance score $SCORE below S-tier threshold (85)\"\n            exit 1\n          fi\n\n          echo \"✅ Performance score: $SCORE (S-tier achieved)\"\n\n      - name: Upload real-world test results\n        uses: actions/upload-artifact@v4\n        with:\n          name: real-world-performance\n          path: |\n            real-world-results.json\n            performance-score-report.html\n            core-web-vitals-report.json\n          retention-days: 30\n\n  performance-monitoring-summary:\n    runs-on: ubuntu-latest\n    needs: [performance-benchmarks, bundle-size-performance, memory-leak-detection, real-world-performance-validation]\n    if: always()\n    steps:\n      - name: Download all performance artifacts\n        uses: actions/download-artifact@v4\n        with:\n          path: performance-artifacts\n\n      - name: Generate comprehensive performance report\n        run: |\n          node scripts/generate-performance-summary.js performance-artifacts/ > PERFORMANCE_SUMMARY.md\n\n      - name: Upload performance summary\n        uses: actions/upload-artifact@v4\n        with:\n          name: performance-summary\n          path: PERFORMANCE_SUMMARY.md\n          retention-days: 90\n\n      - name: Update performance badge\n        if: github.ref == 'refs/heads/main'\n        run: |\n          # Update performance badge in README\n          node scripts/update-performance-badge.js\n\n      - name: Send performance alert if needed\n        if: failure() && github.ref == 'refs/heads/main'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const issue = {\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              title: '🚨 Performance Monitoring Alert - S-Tier Standards Not Met',\n              body: `## Performance Monitoring Alert\n\n              **Date**: ${new Date().toISOString()}\n              **Commit**: ${context.sha}\n              **Build**: [${context.runId}](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n\n              ⚠️ The automated performance monitoring detected that one or more performance targets have not been met.\n\n              ### Performance Requirements (S-Tier):\n              - Render time: <16ms (60fps)\n              - Memory leak: <1MB\n              - Frame rate: >55fps\n              - Bundle size: <30KB total\n              - Performance score: >85\n\n              ### Required Actions:\n              1. Review the detailed performance reports in the workflow artifacts\n              2. Identify and fix performance regressions\n              3. Run local performance tests to verify fixes\n              4. Ensure all S-tier performance targets are met\n\n              ### Resources:\n              - [Performance Optimization Guide](./docs/performance.md)\n              - [Bundle Size Guidelines](./docs/bundle-optimization.md)\n              - [Memory Leak Prevention](./docs/memory-management.md)\n\n              This issue will be automatically closed when performance scores return to S-tier levels.`,\n              labels: ['performance', 'priority-high', 's-tier', 'automated']\n            };\n\n            github.rest.issues.create(issue);\n",
        "start_byte": 0,
        "end_byte": 15013,
        "start_point": {
          "row": 0,
          "column": 0
        },
        "end_point": {
          "row": 448,
          "column": 0
        },
        "state": null,
        "updated_at": null,
        "created_at": null,
        "dependents": [],
        "dependencies": [],
        "api_group_version": {
          "alias": "github-actions",
          "group": "github/actions",
          "version": "1.0.0",
          "provisioner": "GithubActions",
          "status": "AVAILABLE"
        },
        "generated_summary": null
      },
      {
        "id": "3772986a-b006-4089-b4f2-fbdf8f2f7c63",
        "provider": "GithubActions",
        "provisioner": "GithubActions",
        "language": "yaml",
        "key": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/bundle-size.yml:bundle-size-check",
        "digest": 11854322641142830135,
        "references": [],
        "kind": "workflow",
        "type": null,
        "name": "Bundle Size Check",
        "config": null,
        "document_uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/bundle-size.yml",
        "code": "name: Bundle Size Check\non:\n  pull_request:\n    branches:\n      - main\n  push:\n    branches:\n      - main\n\njobs:\n  bundle-size:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build bundles\n        run: |\n          npm run build:standard\n          npm run build:optimized\n          npm run build:modular\n\n      - name: Analyze bundle sizes\n        id: analyze\n        run: |\n          # Core bundle check (<15KB)\n          CORE_SIZE=$(stat -f%z dist/core.min.js 2>/dev/null || stat -c%s dist/core.min.js)\n          CORE_SIZE_KB=$((CORE_SIZE / 1024))\n          echo \"core_size=$CORE_SIZE_KB\" >> $GITHUB_OUTPUT\n          \n          # Animation bundle check (<10KB)\n          ANIM_SIZE=$(stat -f%z dist/animations.min.js 2>/dev/null || stat -c%s dist/animations.min.js)\n          ANIM_SIZE_KB=$((ANIM_SIZE / 1024))\n          echo \"anim_size=$ANIM_SIZE_KB\" >> $GITHUB_OUTPUT\n          \n          # Advanced bundle check (<8KB)\n          ADV_SIZE=$(stat -f%z dist/advanced.min.js 2>/dev/null || stat -c%s dist/advanced.min.js)\n          ADV_SIZE_KB=$((ADV_SIZE / 1024))\n          echo \"adv_size=$ADV_SIZE_KB\" >> $GITHUB_OUTPUT\n          \n          # Total bundle check (<30KB)\n          TOTAL_SIZE=$((CORE_SIZE + ANIM_SIZE + ADV_SIZE))\n          TOTAL_SIZE_KB=$((TOTAL_SIZE / 1024))\n          echo \"total_size=$TOTAL_SIZE_KB\" >> $GITHUB_OUTPUT\n          \n          # Check if sizes meet S-tier requirements\n          if [ $CORE_SIZE_KB -gt 15 ]; then\n            echo \"❌ Core bundle exceeds 15KB limit: ${CORE_SIZE_KB}KB\" >> bundle-report.txt\n            echo \"core_status=failed\" >> $GITHUB_OUTPUT\n          else\n            echo \"✅ Core bundle: ${CORE_SIZE_KB}KB (limit: 15KB)\" >> bundle-report.txt\n            echo \"core_status=passed\" >> $GITHUB_OUTPUT\n          fi\n          \n          if [ $ANIM_SIZE_KB -gt 10 ]; then\n            echo \"❌ Animation bundle exceeds 10KB limit: ${ANIM_SIZE_KB}KB\" >> bundle-report.txt\n            echo \"anim_status=failed\" >> $GITHUB_OUTPUT\n          else\n            echo \"✅ Animation bundle: ${ANIM_SIZE_KB}KB (limit: 10KB)\" >> bundle-report.txt\n            echo \"anim_status=passed\" >> $GITHUB_OUTPUT\n          fi\n          \n          if [ $ADV_SIZE_KB -gt 8 ]; then\n            echo \"❌ Advanced bundle exceeds 8KB limit: ${ADV_SIZE_KB}KB\" >> bundle-report.txt\n            echo \"adv_status=failed\" >> $GITHUB_OUTPUT\n          else\n            echo \"✅ Advanced bundle: ${ADV_SIZE_KB}KB (limit: 8KB)\" >> bundle-report.txt\n            echo \"adv_status=passed\" >> $GITHUB_OUTPUT\n          fi\n          \n          if [ $TOTAL_SIZE_KB -gt 30 ]; then\n            echo \"❌ Total bundle size exceeds 30KB limit: ${TOTAL_SIZE_KB}KB\" >> bundle-report.txt\n            echo \"total_status=failed\" >> $GITHUB_OUTPUT\n            exit 1\n          else\n            echo \"✅ Total bundle size: ${TOTAL_SIZE_KB}KB (limit: 30KB)\" >> bundle-report.txt\n            echo \"total_status=passed\" >> $GITHUB_OUTPUT\n          fi\n\n      - name: Generate detailed bundle report\n        run: |\n          npm install -g webpack-bundle-analyzer\n          npm run build:analyze\n          \n      - name: Upload bundle analysis\n        uses: actions/upload-artifact@v4\n        with:\n          name: bundle-analysis\n          path: |\n            dist/bundle-stats.html\n            bundle-report.txt\n          retention-days: 7\n\n      - name: Comment PR with bundle sizes\n        if: github.event_name == 'pull_request'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            const report = fs.readFileSync('bundle-report.txt', 'utf8');\n            \n            const comment = `## 📦 Bundle Size Report\n            \n            ${report}\n            \n            ### S-Tier Bundle Requirements:\n            - Core: <15KB\n            - Animations: <10KB  \n            - Advanced: <8KB\n            - **Total: <30KB**\n            \n            [View detailed analysis](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;\n            \n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: comment\n            });",
        "start_byte": 0,
        "end_byte": 4522,
        "start_point": {
          "row": 0,
          "column": 0
        },
        "end_point": {
          "row": 127,
          "column": 15
        },
        "state": null,
        "updated_at": null,
        "created_at": null,
        "dependents": [],
        "dependencies": [],
        "api_group_version": {
          "alias": "github-actions",
          "group": "github/actions",
          "version": "1.0.0",
          "provisioner": "GithubActions",
          "status": "AVAILABLE"
        },
        "generated_summary": null
      },
      {
        "id": "708251fb-6f54-499c-90ec-7c9e7c199645",
        "provider": "GithubActions",
        "provisioner": "GithubActions",
        "language": "yaml",
        "key": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/publish.yml:auto-version-bump-and-publish-to-npm",
        "digest": 12328668948563773741,
        "references": [],
        "kind": "workflow",
        "type": null,
        "name": "Auto Version Bump and Publish to NPM",
        "config": null,
        "document_uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/publish.yml",
        "code": "name: Auto Version Bump and Publish to NPM\n\non:\n  push:\n    branches: [main]\n\npermissions:\n  contents: write\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n      # Checkout with full history for version bumping\n      - uses: actions/checkout@v4\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          fetch-depth: 0\n\n      # Setup Bun environment\n      - name: Setup Bun\n        uses: oven-sh/setup-bun@v1\n        with:\n          bun-version: latest\n\n      # Setup Node.js for npm publishing\n      - name: Setup Node.js for npm\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          registry-url: 'https://registry.npmjs.org'\n\n      # Install dependencies with Bun\n      - name: Install dependencies\n        run: bun install --frozen-lockfile\n\n      # Run tests to ensure quality\n      - name: Run tests\n        run: bun run test:ci\n\n      # Run type checking\n      - name: Type check\n        run: bun run type-check\n\n      # Run linting with OXC\n      - name: Lint code with OXC\n        run: bun run lint:ci\n\n      # Build the package\n      - name: Build package\n        run: bun run build\n\n      # Automated version bump based on commit messages\n      - name: Automated Version Bump\n        id: version-bump\n        uses: phips28/gh-action-bump-version@master\n        with:\n          major-wording: 'BREAKING CHANGE,major'\n          minor-wording: 'feat,feature,minor'\n          patch-wording: 'fix,patch,bugfix,chore,docs,style,refactor,perf,test'\n          tag-prefix: 'v'\n          commit-message: 'CI: bumps version to {{version}} [skip ci]'\n          skip-tag: 'true' # We'll handle tagging and release creation together\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n      # Publish to npm (using npm for publishing)\n      - name: Publish to npm\n        run: npm publish --access public\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}\n\n      # Create git tag and GitHub release after successful publish\n      - name: Create git tag and GitHub release\n        run: |\n          VERSION=$(bun --print \"require('./package.json').version\")\n          echo \"Creating release for version $VERSION\"\n\n          # Create and push tag\n          git tag \"v$VERSION\"\n          git push origin \"v$VERSION\"\n\n          # Create GitHub release using gh CLI\n          gh release create \"v$VERSION\" \\\n            --title \"Release v$VERSION\" \\\n            --notes \"Automated release v$VERSION\n            \n            Changes in this version:\n            - Check the commit history for details\n            - Published to npm as liquidify@$VERSION\"\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
        "start_byte": 0,
        "end_byte": 2692,
        "start_point": {
          "row": 0,
          "column": 0
        },
        "end_point": {
          "row": 92,
          "column": 0
        },
        "state": null,
        "updated_at": null,
        "created_at": null,
        "dependents": [],
        "dependencies": [],
        "api_group_version": {
          "alias": "github-actions",
          "group": "github/actions",
          "version": "1.0.0",
          "provisioner": "GithubActions",
          "status": "AVAILABLE"
        },
        "generated_summary": null
      },
      {
        "id": "6690f929-e041-4608-94c6-e6778bba8667",
        "provider": "GithubActions",
        "provisioner": "GithubActions",
        "language": "yaml",
        "key": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/performance-testing.yml:performance-testing",
        "digest": 4075004114494780325,
        "references": [],
        "kind": "workflow",
        "type": null,
        "name": "Performance Testing",
        "config": null,
        "document_uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/workflows/performance-testing.yml",
        "code": "name: Performance Testing\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n  schedule:\n    # Run performance tests daily at 3 AM UTC\n    - cron: '0 3 * * *'\n\njobs:\n  performance-benchmarks:\n    runs-on: ubuntu-latest\n    timeout-minutes: 30\n\n    strategy:\n      matrix:\n        node-version: [18, 20]\n        test-suite:\n          - component-performance\n          - memory-leak-detection\n          - bundle-size-analysis\n          - real-world-performance\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Need full history for performance comparison\n\n      - name: Setup Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node-version }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: |\n          npm ci\n          npm install -g clinic autocannon\n\n      - name: Build project for performance testing\n        run: |\n          npm run build:production\n          npm run build:modular\n\n      - name: Setup performance testing environment\n        run: |\n          # Enable garbage collection for memory testing\n          export NODE_OPTIONS=\"--expose-gc --max-old-space-size=4096\"\n\n          # Create performance test results directory\n          mkdir -p performance-results\n\n          # Install additional performance tools\n          npm install -g lighthouse-ci puppeteer\n\n      - name: Run Component Performance Benchmarks\n        if: matrix.test-suite == 'component-performance'\n        run: |\n          echo \"🚀 Running component performance benchmarks...\"\n          npm run test:performance:components\n\n          # Store results\n          cp test-results/performance-*.json performance-results/\n        env:\n          NODE_OPTIONS: \"--expose-gc --max-old-space-size=4096\"\n\n      - name: Run Memory Leak Detection\n        if: matrix.test-suite == 'memory-leak-detection'\n        run: |\n          echo \"🧠 Running memory leak detection...\"\n          npm run test:performance:memory\n\n          # Generate memory profiling report\n          node scripts/memory-profiler.js > performance-results/memory-report.json\n        env:\n          NODE_OPTIONS: \"--expose-gc --max-old-space-size=4096\"\n\n      - name: Run Bundle Size Analysis\n        if: matrix.test-suite == 'bundle-size-analysis'\n        run: |\n          echo \"📦 Analyzing bundle performance...\"\n          npm run bundle:budget:enforce\n\n          # Generate bundle analysis report\n          npm run bundle:analyze:performance > performance-results/bundle-analysis.json\n\n          # Check for performance regressions\n          node scripts/bundle-performance-check.js\n\n      - name: Run Real-World Performance Tests\n        if: matrix.test-suite == 'real-world-performance'\n        run: |\n          echo \"🌍 Running real-world performance scenarios...\"\n\n          # Start test server\n          npm run build-storybook\n          npx http-server storybook-static -p 8080 --silent &\n          sleep 10\n\n          # Run Lighthouse performance tests\n          npm install -g @lhci/cli\n          lhci autorun --config=lighthouserc.performance.js\n\n          # Run load testing\n          autocannon -c 10 -d 30 http://localhost:8080 > performance-results/load-test.json\n\n      - name: Analyze Performance Results\n        run: |\n          echo \"📊 Analyzing performance results...\"\n          node scripts/analyze-performance-results.js performance-results/\n\n          # Generate performance score\n          PERF_SCORE=$(node scripts/calculate-performance-score.js)\n          echo \"Performance Score: $PERF_SCORE\"\n          echo \"PERFORMANCE_SCORE=$PERF_SCORE\" >> $GITHUB_ENV\n\n      - name: Check Performance Thresholds\n        run: |\n          echo \"🎯 Checking S-Tier performance thresholds...\"\n\n          # S-Tier requirements:\n          # - Overall performance score ≥ 90\n          # - No memory leaks\n          # - Bundle size < 30KB\n          # - Render time < 16ms\n\n          VIOLATIONS=\"\"\n\n          if [ \"$PERFORMANCE_SCORE\" -lt 90 ]; then\n            VIOLATIONS=\"$VIOLATIONS\\n❌ Performance score ($PERFORMANCE_SCORE) below S-Tier threshold (90)\"\n          fi\n\n          # Check for memory leaks\n          if grep -q \"MEMORY_LEAK_DETECTED\" performance-results/memory-report.json; then\n            VIOLATIONS=\"$VIOLATIONS\\n❌ Memory leaks detected\"\n          fi\n\n          # Check bundle size\n          BUNDLE_SIZE=$(jq -r '.totalSize' performance-results/bundle-analysis.json)\n          if [ \"$BUNDLE_SIZE\" -gt 30720 ]; then  # 30KB in bytes\n            VIOLATIONS=\"$VIOLATIONS\\n❌ Bundle size ($BUNDLE_SIZE bytes) exceeds 30KB limit\"\n          fi\n\n          if [ -n \"$VIOLATIONS\" ]; then\n            echo -e \"Performance violations detected:$VIOLATIONS\"\n            echo \"PERFORMANCE_VIOLATIONS=$VIOLATIONS\" >> $GITHUB_ENV\n            exit 1\n          else\n            echo \"✅ All S-Tier performance thresholds met!\"\n          fi\n\n      - name: Upload Performance Artifacts\n        uses: actions/upload-artifact@v4\n        if: always()\n        with:\n          name: performance-results-${{ matrix.test-suite }}-node${{ matrix.node-version }}\n          path: |\n            performance-results/\n            .lighthouseci/\n          retention-days: 30\n\n      - name: Generate Performance Report\n        run: |\n          echo \"📋 Generating performance report...\"\n          node scripts/generate-performance-report.js \\\n            --results performance-results/ \\\n            --output performance-report.md \\\n            --suite ${{ matrix.test-suite }} \\\n            --node-version ${{ matrix.node-version }}\n\n      - name: Comment PR with Performance Results\n        if: github.event_name == 'pull_request' && matrix.test-suite == 'component-performance'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n\n            let report = '';\n            try {\n              report = fs.readFileSync('performance-report.md', 'utf8');\n            } catch (error) {\n              report = `## ⚡ Performance Test Results\n\n              Performance testing completed for Node.js ${{ matrix.node-version }}.\n\n              **Score**: ${process.env.PERFORMANCE_SCORE || 'N/A'}/100\n\n              ${process.env.PERFORMANCE_VIOLATIONS ?\n                `### ⚠️ Performance Issues:\\n${process.env.PERFORMANCE_VIOLATIONS}` :\n                '### ✅ All performance thresholds met!'}\n\n              [View detailed results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;\n            }\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: report\n            });\n\n  performance-regression-detection:\n    needs: performance-benchmarks\n    runs-on: ubuntu-latest\n    if: always()\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Download Performance Artifacts\n        uses: actions/download-artifact@v4\n        with:\n          pattern: performance-results-*\n          merge-multiple: true\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Detect Performance Regressions\n        run: |\n          echo \"🔍 Detecting performance regressions...\"\n          node scripts/detect-performance-regressions.js \\\n            --current performance-results/ \\\n            --baseline main \\\n            --threshold 10 # 10% regression threshold\n\n      - name: Generate Comprehensive Performance Report\n        run: |\n          echo \"📊 Generating comprehensive performance report...\"\n          node scripts/generate-comprehensive-report.js \\\n            --results performance-results/ \\\n            --output comprehensive-performance-report.md\n\n      - name: Upload Comprehensive Report\n        uses: actions/upload-artifact@v4\n        with:\n          name: comprehensive-performance-report\n          path: comprehensive-performance-report.md\n          retention-days: 90\n\n      - name: Update Performance Badge\n        if: github.ref == 'refs/heads/main'\n        run: |\n          # Calculate overall performance grade\n          OVERALL_SCORE=$(node scripts/calculate-overall-score.js performance-results/)\n\n          if [ \"$OVERALL_SCORE\" -ge 95 ]; then\n            GRADE=\"S-Tier\"\n            COLOR=\"brightgreen\"\n          elif [ \"$OVERALL_SCORE\" -ge 90 ]; then\n            GRADE=\"A\"\n            COLOR=\"green\"\n          elif [ \"$OVERALL_SCORE\" -ge 80 ]; then\n            GRADE=\"B\"\n            COLOR=\"yellow\"\n          else\n            GRADE=\"C\"\n            COLOR=\"orange\"\n          fi\n\n          # Update badge (would integrate with shields.io or similar)\n          echo \"Performance Grade: $GRADE (Score: $OVERALL_SCORE)\"\n\n  memory-leak-monitoring:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'schedule'\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run Extended Memory Leak Tests\n        run: |\n          echo \"🧠 Running extended memory leak monitoring...\"\n          npm run test:memory:extended\n        env:\n          NODE_OPTIONS: \"--expose-gc --max-old-space-size=8192\"\n\n      - name: Generate Memory Health Report\n        run: |\n          node scripts/generate-memory-health-report.js > memory-health-report.json\n\n      - name: Check for Memory Issues\n        run: |\n          if jq -e '.memoryLeaksDetected > 0' memory-health-report.json; then\n            echo \"🚨 Memory leaks detected in scheduled monitoring!\"\n\n            # Create issue for memory leak\n            ISSUE_BODY=\"## 🧠 Memory Leak Alert\n\n            Automated memory leak detection has found potential issues:\n\n            $(cat memory-health-report.json | jq -r '.summary')\n\n            **Action Required:**\n            - Review recent changes to components\n            - Check for event listener cleanup\n            - Verify timer/interval cleanup\n            - Run local memory profiling\n\n            **Report Details:**\n            [View full memory health report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n            \"\n\n            echo \"$ISSUE_BODY\" > issue-body.md\n            echo \"MEMORY_LEAK_DETECTED=true\" >> $GITHUB_ENV\n          fi\n\n      - name: Create Memory Leak Issue\n        if: env.MEMORY_LEAK_DETECTED == 'true'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const fs = require('fs');\n            const issueBody = fs.readFileSync('issue-body.md', 'utf8');\n\n            github.rest.issues.create({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              title: '🚨 Memory Leak Detection Alert',\n              body: issueBody,\n              labels: ['performance', 'memory-leak', 'priority-high', 'automated']\n            });\n\n  performance-dashboard-update:\n    needs: [performance-benchmarks, performance-regression-detection]\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main' && always()\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Download Performance Data\n        uses: actions/download-artifact@v4\n        with:\n          pattern: performance-results-*\n          merge-multiple: true\n\n      - name: Update Performance Dashboard\n        run: |\n          echo \"📈 Updating performance dashboard...\"\n\n          # Generate dashboard data\n          node scripts/generate-dashboard-data.js performance-results/ > dashboard-data.json\n\n          # Update metrics (would typically push to a monitoring service)\n          echo \"Dashboard data generated:\"\n          cat dashboard-data.json\n\n      - name: Store Performance History\n        run: |\n          # Store historical performance data for trend analysis\n          mkdir -p .performance-history\n          TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n          cp dashboard-data.json .performance-history/performance-$TIMESTAMP.json\n\n          # Keep only last 30 days of data\n          find .performance-history -name \"performance-*.json\" -mtime +30 -delete\n\n      - name: Commit Performance History\n        run: |\n          git config --local user.email \"action@github.com\"\n          git config --local user.name \"GitHub Action\"\n          git add .performance-history/\n          git commit -m \"📊 Update performance history [skip ci]\" || exit 0\n          git push\n",
        "start_byte": 0,
        "end_byte": 12785,
        "start_point": {
          "row": 0,
          "column": 0
        },
        "end_point": {
          "row": 388,
          "column": 0
        },
        "state": null,
        "updated_at": null,
        "created_at": null,
        "dependents": [],
        "dependencies": [],
        "api_group_version": {
          "alias": "github-actions",
          "group": "github/actions",
          "version": "1.0.0",
          "provisioner": "GithubActions",
          "status": "AVAILABLE"
        },
        "generated_summary": null
      }
    ],
    "errors": [],
    "warnings": [
      {
        "uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/ISSUE_TEMPLATE/feature_request.yml",
        "message": "Unsupported document type.",
        "details": null
      },
      {
        "uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/ISSUE_TEMPLATE/bug_report.yml",
        "message": "Unsupported document type.",
        "details": null
      },
      {
        "uri": "file:///Users/tuliopinheirocunha/LiqUIdify/.github/ISSUE_TEMPLATE/component_development.yml",
        "message": "Unsupported document type.",
        "details": null
      }
    ]
  }
}